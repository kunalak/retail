{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Stuff"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 10,
>>>>>>> e1cb2ace38236603cefe81ad58c14b801712adb1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>credit_card_number</th><th>receipt_timestamp</th><th>receipt_id</th><th>credit_card_type</th><th>customer</th><th>receipt_total</th><th>store_id</th></tr><tr><td>4526148561475</td><td>2015-02-02T20:53:45+0000</td><td>1444711898511</td><td>Visa</td><td>&lt;null&gt;</td><td>1709.84</td><td>46</td></tr><tr><td>4526148561475</td><td>2015-02-01T20:53:45+0000</td><td>1444714187808</td><td>Visa</td><td>&lt;null&gt;</td><td>1689.71</td><td>164</td></tr><tr><td>346314966079541</td><td>2015-02-04T16:09:48+0000</td><td>1444712094856</td><td>American Express</td><td>&lt;null&gt;</td><td>1319.78</td><td>157</td></tr><tr><td>346314966079541</td><td>2015-02-01T21:39:14+0000</td><td>1444713934069</td><td>American Express</td><td>&lt;null&gt;</td><td>989.9000000000001</td><td>334</td></tr><tr><td>346314966079541</td><td>2015-02-01T21:39:14+0000</td><td>1444711643040</td><td>American Express</td><td>&lt;null&gt;</td><td>419.97</td><td>298</td></tr></table>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL Context"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 18,
>>>>>>> e1cb2ace38236603cefe81ad58c14b801712adb1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now a UDFs, because Spark 1.4 does not have all of the functions we need\n",
    "This does a simple string concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val concat = udf((s1:String, s2:String) => s1 + s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes on the stores and receipts_by_store_date table"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 19,
>>>>>>> e1cb2ace38236603cefe81ad58c14b801712adb1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stores_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"stores\")).\n",
    "      load()\n",
    "      \n",
    "val receipts_by_store_date_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"receipts_by_store_date\")).\n",
    "      load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the sales_by_state\n",
    "1. join receipts_by_store_date to store\n",
    "2. group by state\n",
    "3. sum by receipt_total\n",
    "4. do a select to add the dummy column, rename columns, compute the region and round the totals"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 20,
>>>>>>> e1cb2ace38236603cefe81ad58c14b801712adb1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:32: error: not found: value concat\n",
       "             select(lit(\"dummy\") alias \"dummy\", col(\"state\"), concat( lit(\"US-\"), col(\"state\")) alias \"region\", round(col(\"SUM(receipt_total)\"), lit(2)) alias (\"receipts_total\"))\n",
       "                                                              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val sales_by_state_df = receipts_by_store_date_df.\n",
    "      join(stores_df, stores_df(\"store_id\") === receipts_by_store_date_df(\"store_id\")).\n",
    "      groupBy(stores_df(\"state\")).\n",
    "      sum(\"receipt_total\").\n",
    "      select(lit(\"dummy\") alias \"dummy\", col(\"state\"), concat( lit(\"US-\"), col(\"state\")) alias \"region\", col(\"SUM(receipt_total)\") cast \"Decimal(10,2)\" alias (\"receipts_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------------+\n",
      "|dummy|state|region|receipts_total|\n",
      "+-----+-----+------+--------------+\n",
      "|dummy|   MS| US-MS|    5940679.25|\n",
      "|dummy|   MT| US-MT|    1916823.06|\n",
      "|dummy|   TN| US-TN|   10767153.95|\n",
      "|dummy|   NC| US-NC|   12888588.82|\n",
      "|dummy|   ND| US-ND|    2901100.69|\n",
      "|dummy|   NH| US-NH|    1957237.77|\n",
      "|dummy|   AL| US-AL|    5797108.70|\n",
      "|dummy|   NJ| US-NJ|   11580738.27|\n",
      "|dummy|   TX| US-TX|   25657021.59|\n",
      "|dummy|   NM| US-NM|    2988450.47|\n",
      "+-----+-----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_state_df show 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it to sales_by_state.  First truncate the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr></tr></table>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql truncate retail.sales_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_by_state_df.write.\n",
    "      format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\" -> \"retail\",\n",
    "                  \"table\" -> \"sales_by_state\")).\n",
    "      save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>dummy</th><th>receipts_total</th><th>state</th><th>region</th></tr><tr><td>dummy</td><td>29802833.74</td><td>FL</td><td>US-FL</td></tr><tr><td>dummy</td><td>25657021.59</td><td>TX</td><td>US-TX</td></tr><tr><td>dummy</td><td>20938106.75</td><td>PA</td><td>US-PA</td></tr><tr><td>dummy</td><td>19994663.81</td><td>CA</td><td>US-CA</td></tr><tr><td>dummy</td><td>16736304.37</td><td>NY</td><td>US-NY</td></tr></table>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql select * from retail.sales_by_state limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>credit_card_number</th><th>receipt_timestamp</th><th>receipt_id</th><th>credit_card_type</th><th>customer</th><th>receipt_total</th><th>store_id</th></tr><tr><td>346314966079541</td><td>2015-02-01T21:39:14+0000</td><td>1444719922743</td><td>American Express</td><td>[first_name, last_name, addr1, city, state, zip]</td><td>1249.8700000000001</td><td>46</td></tr><tr><td>4539021895480</td><td>2015-02-01T19:30:32+0000</td><td>1444720126906</td><td>Visa</td><td>[first_name, last_name, addr1, city, state, zip]</td><td>119.94999999999999</td><td>350</td></tr><tr><td>5108670874168396</td><td>2015-02-01T16:34:05+0000</td><td>1444719821354</td><td>MasterCard</td><td>[first_name, last_name, addr1, city, state, zip]</td><td>359.96</td><td>169</td></tr><tr><td>379968187714820</td><td>2015-02-02T01:23:49+0000</td><td>1444720005301</td><td>American Express</td><td>[first_name, last_name, addr1, city, state, zip]</td><td>1069.76</td><td>247</td></tr><tr><td>5383439216820189</td><td>2015-02-02T02:11:49+0000</td><td>1444719806873</td><td>MasterCard</td><td>[first_name, last_name, addr1, city, state, zip]</td><td>679.83</td><td>286</td></tr></table>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql select * from retail.receipts_by_credit_card limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val receipts_by_credit_card_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"receipts_by_credit_card\")).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-Cassandra (Scala 2.10.4)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
